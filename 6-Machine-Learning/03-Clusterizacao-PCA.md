# ğŸ§­ ClusterizaÃ§Ã£o (K-Means) e PCA

## ğŸ¯ Objetivo

Agrupar coletas por similaridade e reduzir dimensionalidade para visualizaÃ§Ã£o e interpretaÃ§Ã£o.

â€” K-Means: agrupa pontos em K clusters.
â€” PCA: projeta dados em componentes principais.

---

## ğŸ“¦ Setup

```bash
pip install scikit-learn pandas numpy matplotlib seaborn
```

---

## ğŸ”¢ Dataset de CaracterÃ­sticas

```python
import pandas as pd
import numpy as np
np.random.seed(42)

n = 120

df = pd.DataFrame({
    'biomassa_g': np.random.normal(280, 40, n),
    'temperatura_c': np.random.normal(20, 2.5, n),
    'salinidade_psu': np.random.normal(35.2, 0.4, n),
    'profundidade_m': np.random.normal(4.0, 1.0, n)
})

print(df.head())
```

---

## ğŸ›ï¸ K-Means (Agrupamento)

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

X = df[['biomassa_g', 'temperatura_c', 'salinidade_psu', 'profundidade_m']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)
kmeans.fit(X_scaled)

labels = kmeans.labels_
df['cluster'] = labels
print(df['cluster'].value_counts())
```

â€” Cada cluster agrupa coletas com padrÃµes semelhantes.

### VisualizaÃ§Ã£o 2D (PCA)

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(7,5))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=df['cluster'], palette='Set2', s=80, edgecolor='black')
plt.title('K-Means clusters (PCA 2D)')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var.)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var.)')
plt.tight_layout(); plt.savefig('clusters_pca.png', dpi=300); plt.show()
```

---

## ğŸ“ PCA: Componentes e Cargas

```python
componentes = pd.DataFrame(pca.components_, columns=X.columns, index=['PC1', 'PC2'])
print('\nCargas dos componentes:')
print(componentes.round(3))

print('\nVariÃ¢ncia explicada:')
print(pca.explained_variance_ratio_.round(3))
```

InterpretaÃ§Ã£o:
- Cargas altas (positivas/negativas) mostram variÃ¡veis que mais pesam em cada PC.

---

## ğŸ” Escolha de K (Elbow Method)

```python
inertias = []
for k in range(2, 8):
    km = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X_scaled)
    inertias.append(km.inertia_)

plt.figure(figsize=(6,4))
plt.plot(range(2,8), inertias, 'o--')
plt.xlabel('NÃºmero de clusters (K)')
plt.ylabel('InÃ©rcia')
plt.title('Elbow Method')
plt.tight_layout(); plt.savefig('elbow.png', dpi=300); plt.show()
```

â€” Ponto de inflexÃ£o sugere bom K.

---

## âœ… Boas PrÃ¡ticas

- Padronizar dados antes de K-Means
- Interpretar PCA com cargas das variÃ¡veis
- Validar clusters (silhouette score)
- Evitar inferÃªncias causais com clusters

â€” Com isso, vocÃª consegue agrupar e visualizar padrÃµes em dados complexos.
