# ðŸŒ¿ ClassificaÃ§Ã£o AvanÃ§ada de EspÃ©cies

## ðŸŽ¯ Objetivo

Aprimorar a classificaÃ§Ã£o de macroalgas com modelos mais robustos: **Random Forest**, **SVM** e boas prÃ¡ticas.

---

## ðŸ“¦ Setup

Instalar dependÃªncias:

```bash
pip install scikit-learn pandas numpy matplotlib seaborn
```

---

## ðŸ§ª Dataset SintÃ©tico

```python
import pandas as pd
import numpy as np

np.random.seed(42)

n = 150
especies = ['Ulva', 'Gracilaria', 'Sargassum']

df = pd.DataFrame({
    'comprimento_cm': np.concatenate([
        np.random.normal(20, 3, n//3),
        np.random.normal(11, 2, n//3),
        np.random.normal(35, 4, n//3)
    ]),
    'largura_cm': np.concatenate([
        np.random.normal(13, 2, n//3),
        np.random.normal(5, 1, n//3),
        np.random.normal(15, 2.5, n//3)
    ]),
    'espessura_mm': np.concatenate([
        np.random.normal(0.7, 0.15, n//3),
        np.random.normal(1.5, 0.2, n//3),
        np.random.normal(3.0, 0.3, n//3)
    ]),
    'especie': np.repeat(especies, n//3)
})
```

---

## ðŸ”§ PreparaÃ§Ã£o + Split

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = df[['comprimento_cm', 'largura_cm', 'espessura_mm']]
y = df['especie']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

---

## ðŸŒ² Random Forest

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

rf = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)
print(f"AcurÃ¡cia RF: {accuracy_score(y_test, y_pred_rf)*100:.1f}%")
print(classification_report(y_test, y_pred_rf))

# ImportÃ¢ncia das features
importancias = pd.Series(rf.feature_importances_, index=X.columns)
print("\nImportÃ¢ncia das caracterÃ­sticas:")
print(importancias.sort_values(ascending=False).round(3))
```

â€” Vantagens: robusto, pouco tuning.
â€” InterpretaÃ§Ã£o: importÃ¢ncia de variÃ¡veis.

---

## ðŸ§­ SVM (Support Vector Machine)

```python
from sklearn.svm import SVC

svm = SVC(kernel='rbf', C=2.0, gamma='scale', probability=True, random_state=42)
svm.fit(X_train_scaled, y_train)

y_pred_svm = svm.predict(X_test_scaled)
print(f"AcurÃ¡cia SVM: {accuracy_score(y_test, y_pred_svm)*100:.1f}%")
print(classification_report(y_test, y_pred_svm))
```

â€” SensÃ­vel Ã  escala â†’ usar StandardScaler.
â€” Kernel RBF captura nÃ£o-linearidades.

---

## ðŸ”„ ValidaÃ§Ã£o Cruzada

```python
from sklearn.model_selection import cross_val_score

scores_rf = cross_val_score(rf, X, y, cv=5)
scores_svm = cross_val_score(SVC(kernel='rbf', C=2.0, gamma='scale'), X, y, cv=5)

print(f"RF CV: {scores_rf.mean():.3f} Â± {scores_rf.std():.3f}")
print(f"SVM CV: {scores_svm.mean():.3f} Â± {scores_svm.std():.3f}")
```

---

## ðŸŽ¯ Tuning (Grid Search)

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 4]
}

gs = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, n_jobs=-1)
gs.fit(X_train, y_train)
print(f"Melhor RF: {gs.best_params_}")
print(f"Score (val): {gs.best_score_:.3f}")
```

---

## ðŸ§ª Matriz de ConfusÃ£o

```python
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
            xticklabels=rf.classes_, yticklabels=rf.classes_)
plt.title('Random Forest - Matriz de ConfusÃ£o')
plt.xlabel('Previsto'); plt.ylabel('Real')
plt.tight_layout(); plt.savefig('cm_rf.png', dpi=300); plt.show()
```

---

## âœ… Boas PrÃ¡ticas

- Padronizar escala para SVM
- Usar validaÃ§Ã£o cruzada
- Evitar overfitting (regularizaÃ§Ã£o / limitar profundidade)
- Medir precisÃ£o, recall e F1 por classe
- Interpretar importÃ¢ncia de variÃ¡veis

â€” PrÃ³xima: Agrupamento (K-Means) e reduÃ§Ã£o de dimensionalidade (PCA).
