# üìñ Gloss√°rio - Estat√≠stica Aplicada

## A

**Alfa (Œ±)**: N√≠vel de signific√¢ncia, probabilidade de rejeitar H‚ÇÄ quando √© verdadeira (geralmente 0.05).

**ANOVA (Analysis of Variance)**: Teste para comparar m√©dias de tr√™s ou mais grupos.

**Assimetria (Skewness)**: Medida de assimetria da distribui√ß√£o.

**Amostra**: Subconjunto de popula√ß√£o usado para infer√™ncia.

## B

**Beta (Œ≤)**: Probabilidade de n√£o rejeitar H‚ÇÄ quando √© falsa (erro tipo II).

**Boxplot**: Gr√°fico que mostra quartis, mediana e outliers.

## C

**Causalidade**: Rela√ß√£o onde uma vari√°vel causa mudan√ßa em outra.

**Chi-quadrado (œá¬≤)**: Teste para verificar independ√™ncia entre vari√°veis categ√≥ricas.

**Coeficiente de Correla√ß√£o (r)**: Medida de for√ßa e dire√ß√£o da rela√ß√£o linear (-1 a +1).

**Coeficiente de Determina√ß√£o (R¬≤)**: Propor√ß√£o da vari√¢ncia explicada pelo modelo (0 a 1).

**Correla√ß√£o**: Medida de associa√ß√£o entre duas vari√°veis.

**Covari√¢ncia**: Medida de varia√ß√£o conjunta de duas vari√°veis.

**Curtose**: Medida de achatamento da distribui√ß√£o.

## D

**Dados Categ√≥ricos**: Dados em categorias (nominal/ordinal).

**Dados Quantitativos**: Dados num√©ricos (discretos/cont√≠nuos).

**Desvio Padr√£o (œÉ ou s)**: Medida de dispers√£o dos dados em torno da m√©dia.

**Distribui√ß√£o**: Padr√£o de como valores de vari√°vel s√£o espalhados.

**Distribui√ß√£o Normal**: Distribui√ß√£o sim√©trica em forma de sino.

## E

**Erro Padr√£o**: Desvio padr√£o da distribui√ß√£o amostral.

**Erro Tipo I**: Rejeitar H‚ÇÄ quando √© verdadeira (falso positivo).

**Erro Tipo II**: N√£o rejeitar H‚ÇÄ quando √© falsa (falso negativo).

**Estat√≠stica Descritiva**: Resumo e descri√ß√£o de dados.

**Estat√≠stica Inferencial**: Conclus√µes sobre popula√ß√£o a partir de amostra.

**Estimador**: F√≥rmula usada para estimar par√¢metro populacional.

## F

**F de Fisher**: Estat√≠stica usada em ANOVA para comparar vari√¢ncias.

## G

**Graus de Liberdade**: N√∫mero de valores livres para variar em c√°lculo estat√≠stico.

## H

**Histograma**: Gr√°fico de frequ√™ncia de dados cont√≠nuos.

**Hip√≥tese Alternativa (H‚ÇÅ)**: Afirma√ß√£o que queremos provar.

**Hip√≥tese Nula (H‚ÇÄ)**: Afirma√ß√£o de que n√£o h√° efeito/diferen√ßa.

## I

**IC (Intervalo de Confian√ßa)**: Faixa de valores prov√°veis para par√¢metro populacional.

**IQR (Interquartile Range)**: Diferen√ßa entre Q3 e Q1, medida de dispers√£o.

## K

**Kolmogorov-Smirnov**: Teste de normalidade de distribui√ß√£o.

**Kurtosis**: Ver Curtose.

## L

**Linearidade**: Rela√ß√£o proporcional entre vari√°veis.

## M

**Mann-Whitney**: Teste n√£o-param√©trico para comparar dois grupos independentes.

**M√°ximo**: Maior valor em conjunto de dados.

**M√©dia (xÃÑ ou Œº)**: Soma dos valores dividida pelo n√∫mero de observa√ß√µes.

**Mediana**: Valor central quando dados est√£o ordenados.

**M√≠nimo**: Menor valor em conjunto de dados.

**Moda**: Valor mais frequente em conjunto de dados.

## N

**Normalidade**: Propriedade de dados seguirem distribui√ß√£o normal.

**N√≠vel de Confian√ßa**: Probabilidade de IC conter par√¢metro verdadeiro (ex: 95%).

## O

**Outlier**: Valor extremo que destoa do padr√£o.

## P

**p-valor**: Probabilidade de obter resultado observado se H‚ÇÄ for verdadeira.

**Par√¢metro**: Caracter√≠stica num√©rica de popula√ß√£o (Œº, œÉ).

**Percentil**: Valor abaixo do qual certa porcentagem de dados cai.

**Poder do Teste (1-Œ≤)**: Probabilidade de rejeitar H‚ÇÄ quando √© falsa.

**Popula√ß√£o**: Conjunto completo de elementos de interesse.

## Q

**Q1 (Primeiro Quartil)**: Valor que deixa 25% dos dados abaixo.

**Q2 (Segundo Quartil)**: Mediana (50%).

**Q3 (Terceiro Quartil)**: Valor que deixa 75% dos dados abaixo.

## R

**Regress√£o**: T√©cnica para modelar rela√ß√£o entre vari√°veis.

**Regress√£o Linear**: Modelo de rela√ß√£o linear entre vari√°veis.

**Regress√£o M√∫ltipla**: Regress√£o com m√∫ltiplas vari√°veis explicativas.

**Res√≠duo**: Diferen√ßa entre valor observado e previsto.

**R de Pearson**: Coeficiente de correla√ß√£o linear.

## S

**Shapiro-Wilk**: Teste de normalidade sens√≠vel.

**Signific√¢ncia Estat√≠stica**: Resultado improv√°vel de ocorrer por acaso (p < Œ±).

**Spearman (œÅ)**: Coeficiente de correla√ß√£o de postos (n√£o-param√©trico).

## T

**Teste Bicaudal**: Teste que considera desvios em ambas dire√ß√µes.

**Teste de Hip√≥tese**: Procedimento para decidir entre H‚ÇÄ e H‚ÇÅ.

**Teste Param√©trico**: Teste que assume distribui√ß√£o espec√≠fica dos dados.

**Teste t de Student**: Teste para comparar m√©dias.

**Teste Unicaudal**: Teste que considera desvio em uma dire√ß√£o.

## V

**Valor-p**: Ver p-valor.

**Vari√¢ncia (œÉ¬≤ ou s¬≤)**: M√©dia dos desvios quadr√°ticos em rela√ß√£o √† m√©dia.

**Vari√°vel Dependente (Y)**: Vari√°vel que queremos explicar/prever.

**Vari√°vel Independente (X)**: Vari√°vel explicativa/preditora.

## W

**Wilcoxon**: Teste n√£o-param√©trico para dados pareados.

## Z

**z-score**: N√∫mero de desvios padr√£o que valor est√° da m√©dia.

## F√≥rmulas Essenciais

### Estat√≠stica Descritiva
```
M√©dia: xÃÑ = Œ£x / n

Vari√¢ncia: s¬≤ = Œ£(x - xÃÑ)¬≤ / (n-1)

Desvio Padr√£o: s = ‚àös¬≤

Coeficiente de Varia√ß√£o: CV = (s / xÃÑ) √ó 100%
```

### Correla√ß√£o e Regress√£o
```
Correla√ß√£o de Pearson: r = Œ£[(x - xÃÑ)(y - »≥)] / ‚àö[Œ£(x - xÃÑ)¬≤Œ£(y - »≥)¬≤]

Regress√£o Linear: y = a + bx
    b = Œ£[(x - xÃÑ)(y - »≥)] / Œ£(x - xÃÑ)¬≤
    a = »≥ - bxÃÑ

R¬≤: R¬≤ = 1 - (SSres / SStot)
```

### Testes de Hip√≥tese
```
Teste t: t = (xÃÑ - Œº‚ÇÄ) / (s / ‚àön)

Intervalo de Confian√ßa (95%): xÃÑ ¬± t‚ÇÄ.‚ÇÄ‚ÇÇ‚ÇÖ √ó (s / ‚àön)

Tamanho de Efeito (Cohen's d): d = (xÃÑ‚ÇÅ - xÃÑ‚ÇÇ) / s_pooled
```

## Exemplos Pr√°ticos

### Estat√≠stica Descritiva Completa
```python
import pandas as pd
import numpy as np
from scipy import stats

# Dados de temperatura
temperaturas = [22.5, 23.1, 21.8, 24.2, 22.9, 23.5, 22.1, 23.8, 22.6, 23.3]

# Estat√≠sticas b√°sicas
print("M√©dia:", np.mean(temperaturas))
print("Mediana:", np.median(temperaturas))
print("Moda:", stats.mode(temperaturas))
print("Desvio Padr√£o:", np.std(temperaturas, ddof=1))
print("Vari√¢ncia:", np.var(temperaturas, ddof=1))
print("M√≠nimo:", np.min(temperaturas))
print("M√°ximo:", np.max(temperaturas))
print("Amplitude:", np.max(temperaturas) - np.min(temperaturas))

# Quartis
q1 = np.percentile(temperaturas, 25)
q2 = np.percentile(temperaturas, 50)  # Mediana
q3 = np.percentile(temperaturas, 75)
iqr = q3 - q1

print(f"Q1: {q1}, Q2: {q2}, Q3: {q3}, IQR: {iqr}")

# Teste de normalidade
estatistica, p_valor = stats.shapiro(temperaturas)
print(f"Shapiro-Wilk: p-valor = {p_valor:.4f}")
if p_valor > 0.05:
    print("Dados s√£o normalmente distribu√≠dos")
```

### Teste de Hip√≥tese (Teste t)
```python
import scipy.stats as stats

# Dados: temperatura antes e depois de interven√ß√£o
antes = [22.5, 23.1, 21.8, 24.2, 22.9]
depois = [20.1, 19.8, 21.2, 20.5, 19.9]

# Teste t pareado
t_stat, p_valor = stats.ttest_rel(antes, depois)

print(f"Estat√≠stica t: {t_stat:.3f}")
print(f"p-valor: {p_valor:.4f}")

alpha = 0.05
if p_valor < alpha:
    print("H√° diferen√ßa significativa (rejeitar H‚ÇÄ)")
else:
    print("N√£o h√° diferen√ßa significativa (n√£o rejeitar H‚ÇÄ)")

# Tamanho de efeito
d = np.mean(np.array(antes) - np.array(depois)) / np.std(np.array(antes) - np.array(depois), ddof=1)
print(f"Cohen's d: {d:.3f}")
```

### Correla√ß√£o e Regress√£o
```python
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, spearmanr

# Dados
temperatura = [20, 22, 24, 26, 28, 30, 32]
abundancia = [45, 52, 58, 62, 55, 48, 42]

# Correla√ß√£o de Pearson
r_pearson, p_pearson = pearsonr(temperatura, abundancia)
print(f"Correla√ß√£o de Pearson: r = {r_pearson:.3f}, p = {p_pearson:.4f}")

# Correla√ß√£o de Spearman
r_spearman, p_spearman = spearmanr(temperatura, abundancia)
print(f"Correla√ß√£o de Spearman: œÅ = {r_spearman:.3f}, p = {p_spearman:.4f}")

# Regress√£o linear
from scipy.stats import linregress
slope, intercept, r_value, p_value, std_err = linregress(temperatura, abundancia)

print(f"Equa√ß√£o: y = {intercept:.2f} + {slope:.2f}x")
print(f"R¬≤ = {r_value**2:.3f}")

# Plotar
plt.figure(figsize=(10, 6))
plt.scatter(temperatura, abundancia, s=100, alpha=0.6, label='Dados')
plt.plot(temperatura, intercept + slope*np.array(temperatura), 'r-', label='Regress√£o')
plt.xlabel('Temperatura (¬∞C)')
plt.ylabel('Abund√¢ncia')
plt.title(f'Correla√ß√£o: r = {r_pearson:.3f}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('regressao.png', dpi=300, bbox_inches='tight')
```

### ANOVA (Compara√ß√£o de 3+ grupos)
```python
from scipy.stats import f_oneway

# Dados de 3 praias
praia1 = [22.5, 23.1, 22.8, 23.2]
praia2 = [25.1, 24.8, 25.3, 25.0]
praia3 = [21.2, 20.9, 21.5, 21.1]

# ANOVA
F_stat, p_valor = f_oneway(praia1, praia2, praia3)

print(f"Estat√≠stica F: {F_stat:.3f}")
print(f"p-valor: {p_valor:.4f}")

if p_valor < 0.05:
    print("H√° diferen√ßa significativa entre as praias")
else:
    print("N√£o h√° diferen√ßa significativa entre as praias")
```

## Guia de Escolha de Teste

| Situa√ß√£o | Teste Recomendado |
|----------|-------------------|
| Comparar 2 grupos independentes (dados normais) | Teste t independente |
| Comparar 2 grupos pareados (dados normais) | Teste t pareado |
| Comparar 2 grupos (dados n√£o-normais) | Mann-Whitney |
| Comparar 3+ grupos (dados normais) | ANOVA |
| Comparar 3+ grupos (dados n√£o-normais) | Kruskal-Wallis |
| Correla√ß√£o linear | Pearson |
| Correla√ß√£o n√£o-linear | Spearman |
| Testar normalidade | Shapiro-Wilk |
| Vari√°veis categ√≥ricas | Chi-quadrado |

---

üí° **Dica**: Sempre verifique as premissas do teste antes de aplic√°-lo!
